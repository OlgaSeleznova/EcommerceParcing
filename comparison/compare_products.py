"""
Product comparison module using Langchain.

This module compares the top 3 rated products based on criteria generated by Langchain.
"""
import os
import sys
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
from operator import itemgetter
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import DATA_PATHS, LOGGING_CONFIG, LLM_CONFIG

# Configure logging
logging.basicConfig(
    level=getattr(logging, LOGGING_CONFIG["level"]),
    format=LOGGING_CONFIG["format"],
    handlers=[
        logging.FileHandler(LOGGING_CONFIG["file"]),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("product_comparison")

# Set output paths
COMPARISON_OUTPUT_PATH = os.path.join(os.path.dirname(DATA_PATHS["processed_data"]), "product_comparison.json")

def load_products(file_path: str = DATA_PATHS["processed_data"]) -> List[Dict[str, Any]]:
    """
    Load products from the processed data file.
    
    Args:
        file_path (str): Path to the processed data file
        
    Returns:
        List[Dict[str, Any]]: List of processed products
    """
    try:
        if not os.path.exists(file_path):
            # Try to load from scraped data if processed data doesn't exist
            file_path = DATA_PATHS["scraped_data"]
            if not os.path.exists(file_path):
                logger.error(f"No product data found at {file_path}")
                return []
                
        with open(file_path, 'r') as f:
            products = json.load(f)
            
        logger.info(f"Loaded {len(products)} products from {file_path}")
        return products
        
    except Exception as e:
        logger.error(f"Error loading products from {file_path}: {str(e)}")
        return []

def get_top_rated_products(products: List[Dict[str, Any]], top_n: int = 3) -> List[Dict[str, Any]]:
    """
    Get the top N products with the highest rating.
    
    Args:
        products (List[Dict[str, Any]]): List of products
        top_n (int): Number of top products to return
        
    Returns:
        List[Dict[str, Any]]: Top N rated products
    """
    # Filter out products without a rating
    rated_products = [p for p in products if p.get("rating")]
    
    # Convert ratings to float (they might be strings)
    for product in rated_products:
        try:
            product["rating"] = float(product["rating"])
        except (ValueError, TypeError):
            product["rating"] = 0.0
    
    # Sort by rating (descending)
    sorted_products = sorted(rated_products, key=lambda x: x.get("rating", 0.0), reverse=True)
    
    # Return top N products
    top_products = sorted_products[:top_n]
    
    logger.info(f"Found top {len(top_products)} rated products")
    for i, product in enumerate(top_products):
        logger.info(f"  {i+1}. {product.get('title')} (Rating: {product.get('rating')})")
    
    return top_products

def init_langchain_llm():
    """
    Initialize the Langchain LLM with OpenAI.
    
    Returns:
        ChatOpenAI: Initialized LLM
    """
    api_key = os.environ.get("OPENAI_API_KEY") or LLM_CONFIG["OpenAI"]["api_key"]
    
    if not api_key:
        logger.error("No OpenAI API key found. Set OPENAI_API_KEY environment variable.")
        return None
    
    return ChatOpenAI(
        model=LLM_CONFIG["OpenAI"]["model"],
        temperature=LLM_CONFIG["OpenAI"]["temperature"],
        api_key=api_key
    )

def generate_comparison_criteria(llm, products: List[Dict[str, Any]]) -> List[str]:
    """
    Generate comparison criteria (questions) using Langchain.
    
    Args:
        llm: Langchain LLM
        products (List[Dict[str, Any]]): List of products to compare
        
    Returns:
        List[str]: List of comparison criteria questions
    """
    product_descriptions = []
    for i, product in enumerate(products):
        title = product.get("title", f"Product {i+1}")
        description = product.get("description", "")
        features = product.get("features", [])
        features_text = "\n".join([f"- {feature}" for feature in features])
        
        product_descriptions.append(f"Product {i+1}: {title}\nDescription: {description}\nFeatures:\n{features_text}")
    
    all_products_text = "\n\n".join(product_descriptions)
    
    prompt_template = PromptTemplate(
        input_variables=["products"],
        template="""You are a product comparison specialist. 
        Based on the following product descriptions, generate 5 important comparison criteria that would help a customer decide which product is best.
        Frame each criterion as a question that asks which product is best for that specific criterion.
        
        Products:
        {products}
        
        Output exactly 5 comparison criteria questions, one per line, without numbering or additional text."""
    )
    
    chain = LLMChain(llm=llm, prompt=prompt_template)
    result = chain.run(products=all_products_text)
    
    # Parse the result to get individual questions
    questions = [q.strip() for q in result.strip().split("\n") if q.strip()]
    
    # Ensure we have exactly 5 questions
    if len(questions) > 5:
        questions = questions[:5]
    elif len(questions) < 5:
        # Fill in with generic questions if needed
        generic_questions = [
            "Which product offers the best performance for its price?",
            "Which product has the most user-friendly features?",
            "Which product provides the best long-term value?",
            "Which product has the most versatile functionality?",
            "Which product is most suitable for professional use?"
        ]
        questions.extend(generic_questions[:(5 - len(questions))])
    
    logger.info("Generated 5 comparison criteria questions:")
    for i, question in enumerate(questions):
        logger.info(f"  {i+1}. {question}")
    
    return questions

def generate_comparison_answers(llm, products: List[Dict[str, Any]], criteria: List[str]) -> List[Dict[str, Any]]:
    """
    Generate answers for each comparison criterion.
    
    Args:
        llm: Langchain LLM
        products (List[Dict[str, Any]]): List of products to compare
        criteria (List[str]): List of comparison criteria questions
        
    Returns:
        List[Dict[str, Any]]: List of comparison results
    """
    product_descriptions = []
    for i, product in enumerate(products):
        title = product.get("title", f"Product {i+1}")
        description = product.get("description", "")
        features = product.get("features", [])
        features_text = "\n".join([f"- {feature}" for feature in features])
        
        product_descriptions.append(f"Product {i+1}: {title}\nDescription: {description}\nFeatures:\n{features_text}")
    
    all_products_text = "\n\n".join(product_descriptions)
    
    comparison_results = []
    
    for criterion in criteria:
        prompt_template = PromptTemplate(
            input_variables=["criterion", "products"],
            template="""You are a product comparison specialist. Based on the following product descriptions, 
            determine which product is best for the given criterion. Explain your reasoning in 2-3 sentences.
            
            Criterion: {criterion}
            
            Products:
            {products}
            
            First state which product is best for this criterion (Product 1, Product 2, or Product 3).
            Then explain why in 2-3 sentences."""
        )
        
        chain = LLMChain(llm=llm, prompt=prompt_template)
        result = chain.run(criterion=criterion, products=all_products_text)
        
        # Parse the result to extract the winner and explanation
        lines = result.strip().split("\n")
        winner = ""
        explanation = ""
        
        for line in lines:
            if "Product 1" in line or "Product 2" in line or "Product 3" in line:
                winner = line.strip()
            else:
                if explanation:
                    explanation += " " + line.strip()
                else:
                    explanation = line.strip()
        
        # If parsing failed, use the whole result
        if not winner:
            winner = "Unable to determine clear winner"
        if not explanation:
            explanation = result.strip()
        
        comparison_results.append({
            "criterion": criterion,
            "winner": winner,
            "explanation": explanation
        })
        
        logger.info(f"Generated answer for criterion: {criterion}")
        logger.info(f"  Winner: {winner}")
        logger.info(f"  Explanation: {explanation}")
    
    return comparison_results

def prepare_comparison_output(products: List[Dict[str, Any]], comparison_results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Prepare the final comparison output.
    
    Args:
        products (List[Dict[str, Any]]): List of products being compared
        comparison_results (List[Dict[str, Any]]): Comparison results
        
    Returns:
        Dict[str, Any]: Structured comparison output
    """
    product_info = []
    for i, product in enumerate(products):
        product_info.append({
            "id": product.get("id", f"product-{i+1}"),
            "title": product.get("title", f"Product {i+1}"),
            "rating": product.get("rating", 0),
            "price": product.get("price", "N/A"),
            "url": product.get("url", ""),
            "summary": product.get("summary", ""),
            "tagline": product.get("tagline", "")
        })
    
    return {
        "timestamp": Path(DATA_PATHS["processed_data"]).stat().st_mtime,
        "products": product_info,
        "comparison_criteria": [result["criterion"] for result in comparison_results],
        "comparison_results": comparison_results
    }

def save_comparison_results(comparison_data: Dict[str, Any]) -> bool:
    """
    Save comparison results to a JSON file.
    
    Args:
        comparison_data (Dict[str, Any]): Comparison data to save
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        with open(COMPARISON_OUTPUT_PATH, 'w') as f:
            json.dump(comparison_data, f, indent=2)
        
        logger.info(f"Saved comparison results to {COMPARISON_OUTPUT_PATH}")
        return True
    
    except Exception as e:
        logger.error(f"Error saving comparison results: {str(e)}")
        return False

def compare_products(use_mock: bool = False) -> Dict[str, Any]:
    """
    Compare the top 3 rated products using Langchain.
    
    Args:
        use_mock (bool): Whether to use mock data for testing
        
    Returns:
        Dict[str, Any]: Comparison results
    """
    # Load products
    products = load_products()
    if not products:
        logger.error("No products found to compare")
        return {}
    
    # Get top 3 rated products
    top_products = get_top_rated_products(products)
    if len(top_products) < 2:
        logger.error(f"Not enough rated products for comparison (found {len(top_products)}, need at least 2)")
        return {}
    
    if use_mock:
        # Use mock data for testing
        logger.info("Using mock data for comparison")
        
        mock_criteria = [
            "Which product offers the best performance for its price?",
            "Which product has the most user-friendly features?",
            "Which product provides the best long-term value?",
            "Which product has the most versatile functionality?",
            "Which product is most suitable for professional use?"
        ]
        
        mock_results = []
        for i, criterion in enumerate(mock_criteria):
            # Alternate winners for variety
            winner_idx = i % len(top_products)
            winner = f"Product {winner_idx + 1}"
            
            mock_results.append({
                "criterion": criterion,
                "winner": f"{winner} is the best for this criterion",
                "explanation": f"{winner} excels in this area because of its superior specifications and features. It provides exceptional value and performance compared to the alternatives."
            })
        
        comparison_data = prepare_comparison_output(top_products, mock_results)
        save_comparison_results(comparison_data)
        return comparison_data
    
    # Initialize Langchain LLM
    llm = init_langchain_llm()
    if not llm:
        logger.error("Failed to initialize Langchain LLM. Check your OpenAI API key.")
        return {}
    
    # Generate comparison criteria
    criteria = generate_comparison_criteria(llm, top_products)
    
    # Generate comparison answers
    results = generate_comparison_answers(llm, top_products, criteria)
    
    # Prepare and save the final output
    comparison_data = prepare_comparison_output(top_products, results)
    save_comparison_results(comparison_data)
    
    return comparison_data

if __name__ == "__main__":
    # Determine whether to use mock data based on API key availability
    api_key = os.environ.get("OPENAI_API_KEY") or LLM_CONFIG["OpenAI"].get("api_key")
    use_mock = not api_key or api_key == "None"
    
    compare_products(use_mock=use_mock)
