"""
Product comparison module using Langchain.

This module compares the top 3 rated products based on criteria generated by Langchain.
"""
import os
import sys
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
from operator import itemgetter
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# Load environment variables from config.env file
load_dotenv(dotenv_path='config.env')

# Import config
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import DATA_PATHS, LOGGING_CONFIG, LLM_CONFIG

# Define comparison output path
COMPARISON_OUTPUT_PATH = str(Path(DATA_PATHS["processed_data"]).parent / "product_comparison.json")

# Configure logging
logging.basicConfig(
    level=getattr(logging, LOGGING_CONFIG["level"]),
    format=LOGGING_CONFIG["format"],
    handlers=[
        logging.FileHandler(LOGGING_CONFIG["file"]),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("product_comparison")

# Set output paths
COMPARISON_OUTPUT_PATH = os.path.join(os.path.dirname(DATA_PATHS["processed_data"]), "product_comparison.json")

def load_products(file_path: str = DATA_PATHS["processed_data"]) -> List[Dict[str, Any]]:
    """
    Load products from the processed data file.
    
    Args:
        file_path (str): Path to the processed data file
        
    Returns:
        List[Dict[str, Any]]: List of processed products
    """
    try:
        if not os.path.exists(file_path):
            # Try to load from scraped data if processed data doesn't exist
            file_path = DATA_PATHS["scraped_data"]
            if not os.path.exists(file_path):
                logger.error(f"No product data found at {file_path}")
                return []
                
        with open(file_path, 'r') as f:
            products = json.load(f)
            
        logger.info(f"Loaded {len(products)} products from {file_path}")
        return products
        
    except Exception as e:
        logger.error(f"Error loading products from {file_path}: {str(e)}")
        return []

def get_top_rated_products(products: List[Dict[str, Any]], top_n: int = 3) -> List[Dict[str, Any]]:
    """
    Get the top N products with the highest rating.
    
    Args:
        products (List[Dict[str, Any]]): List of products
        top_n (int): Number of top products to return
        
    Returns:
        List[Dict[str, Any]]: Top N rated products
    """
    # Filter out products without a rating
    rated_products = [p for p in products if p.get("rating")]
    
    # Convert ratings to float (they might be strings)
    for product in rated_products:
        try:
            product["rating"] = float(product["rating"])
        except (ValueError, TypeError):
            product["rating"] = 0.0
    
    # Sort by rating (descending)
    sorted_products = sorted(rated_products, key=lambda x: x.get("rating", 0.0), reverse=True)
    
    # Return top N products
    top_products = sorted_products[:top_n]
    
    logger.info(f"Found top {len(top_products)} rated products")
    for i, product in enumerate(top_products):
        logger.info(f"  {i+1}. {product.get('title')} (Rating: {product.get('rating')})")
    
    return top_products

def init_langchain_llm():
    """
    Initialize the Langchain LLM with OpenAI.
    
    Returns:
        ChatOpenAI: Initialized LLM
    """
    # First check environment variable, then config
    api_key = os.getenv("OPENAI_API_KEY") or LLM_CONFIG["OpenAI"]["api_key"]
    
    if not api_key:
        logger.error("No OpenAI API key found. Set OPENAI_API_KEY in config.env file or environment variable.")
        return None
    
    logger.info("Initializing LangChain with OpenAI")
    return ChatOpenAI(
        model=LLM_CONFIG["OpenAI"]["model"],
        temperature=LLM_CONFIG["OpenAI"]["temperature"],
        max_tokens=LLM_CONFIG["OpenAI"]["max_tokens"],
        api_key=api_key
    )

def generate_comparison_criteria(llm, products: List[Dict[str, Any]]) -> List[str]:
    """
    Generate comparison criteria (questions) using Langchain.
    
    Args:
        llm: Langchain LLM
        products (List[Dict[str, Any]]): List of products to compare
        
    Returns:
        List[str]: List of comparison criteria questions
    """
    product_descriptions = []
    for i, product in enumerate(products):
        title = product.get("title", f"Product {i+1}")
        description = product.get("description", "")
        features = product.get("features", [])
        features_text = "\n".join([f"- {feature}" for feature in features])
        
        product_descriptions.append(f"Product {i+1}: {title}\nDescription: {description}\nFeatures:\n{features_text}")
    
    all_products_text = "\n\n".join(product_descriptions)
    
    # Get system message and prompt from config
    config_path = LLM_CONFIG["PROMPTS"]["product_comparison"]["questions"]
    system_msg = config_path["system_msg"]
    prompt = config_path["prompt"].format(products=all_products_text)
    
    # Create messages for the LLM
    messages = [
        SystemMessage(content=system_msg),
        HumanMessage(content=prompt)
    ]
    
    # Get response from LLM
    result = llm.predict_messages(messages).content
    
    # Parse the result to get individual questions
    questions = [q.strip() for q in result.strip().split("\n") if q.strip()]
    
    logger.info("Generated comparison criteria questions:")
    for i, question in enumerate(questions):
        logger.info(f"  {i+1}. {question}")
    
    return questions


def generate_comparison_answers(llm, products: List[Dict[str, Any]], criteria: List[str]) -> List[Dict[str, Any]]:
    """
    Generate answers for each comparison criterion.
    
    Args:
        llm: Langchain LLM
        products (List[Dict[str, Any]]): List of products to compare
        criteria (List[str]): List of comparison criteria questions
        
    Returns:
        List[Dict[str, Any]]: List of comparison results
    """
    product_descriptions = []
    for i, product in enumerate(products):
        title = product.get("title", f"Product {i+1}")
        description = product.get("description", "")
        features = product.get("features", [])
        features_text = "\n".join([f"- {feature}" for feature in features])
        
        product_descriptions.append(f"Product {i+1}: {title}\nDescription: {description}\nFeatures:\n{features_text}")
    
    all_products_text = "\n\n".join(product_descriptions)
    
    comparison_results = []
    
    for criterion in criteria:
        # Get system message and prompt from config
        config_path = LLM_CONFIG["PROMPTS"]["product_comparison"]["responses"]
        system_msg = config_path["system_msg"]
        prompt = config_path["prompt"].format(
            criterion=criterion, 
            products=all_products_text
        )
        
        # Create messages for the LLM
        messages = [
            SystemMessage(content=system_msg),
            HumanMessage(content=prompt)
        ]
        
        # Get response from LLM
        result = llm.predict_messages(messages).content
        
        # Parse the result to extract the winner and explanation
        lines = result.strip().split("\n")
        winner = ""
        explanation = ""
        
        for line in lines:
            if "Product 1" in line or "Product 2" in line or "Product 3" in line:
                winner = line.strip()
            else:
                if explanation:
                    explanation += " " + line.strip()
                else:
                    explanation = line.strip()
        
        # If parsing failed, use the whole result
        if not winner:
            winner = "Unable to determine clear winner"
        if not explanation:
            explanation = result.strip()
        
        comparison_results.append({
            "criterion": criterion,
            "winner": winner,
            "explanation": explanation
        })
        
        logger.info(f"Generated answer for criterion: {criterion}")
        logger.info(f"  Winner: {winner}")
        logger.info(f"  Explanation: {explanation}")
    
    return comparison_results

def prepare_comparison_output(comparison_results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Prepare the final comparison output with only criteria and results.
    
    Args:
        comparison_results (List[Dict[str, Any]]): Comparison results
        
    Returns:
        Dict[str, Any]: Structured comparison output with only criteria and results
    """
    return {
        "timestamp": Path(DATA_PATHS["processed_data"]).stat().st_mtime,
        "comparison_criteria": [result["criterion"] for result in comparison_results],
        "comparison_results": comparison_results
    }

def save_comparison_results(comparison_data: Dict[str, Any]) -> bool:
    """
    Save comparison results to a JSON file.
    
    Args:
        comparison_data (Dict[str, Any]): Comparison data to save
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        with open(COMPARISON_OUTPUT_PATH, 'w') as f:
            json.dump(comparison_data, f, indent=2)
        
        logger.info(f"Saved comparison results to {COMPARISON_OUTPUT_PATH}")
        return True
    
    except Exception as e:
        logger.error(f"Error saving comparison results: {str(e)}")
        return False

def compare_products(output_path: Optional[str] = None) -> Dict[str, Any]:
    """
    Compare the top 3 rated products using Langchain.
    
    Args:
        output_path (Optional[str]): Custom path to save comparison results
        
    Returns:
        Dict[str, Any]: Comparison results
    """
    # Set custom output path if provided
    global COMPARISON_OUTPUT_PATH
    if output_path:
        COMPARISON_OUTPUT_PATH = output_path
    
    # Load products
    products = load_products()
    if not products:
        logger.error("No products found to compare")
        return {}
    
    # Get top 3 rated products
    top_products = get_top_rated_products(products)
    if len(top_products) < 2:
        logger.error(f"Not enough rated products for comparison (found {len(top_products)}, need at least 2)")
        return {}
    
    # Initialize Langchain LLM
    llm = init_langchain_llm()
    if not llm:
        logger.error("Failed to initialize Langchain LLM. Check your OpenAI API key.")
        return {}
    
    # Generate comparison criteria
    criteria = generate_comparison_criteria(llm, top_products)
    
    # Generate comparison answers
    results = generate_comparison_answers(llm, top_products, criteria)
    
    # Prepare and save the final output
    comparison_data = prepare_comparison_output(results)
    save_comparison_results(comparison_data)
    
    return comparison_data



if __name__ == "__main__":
    # Process all products with default settings
    compare_products()

